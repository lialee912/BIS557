<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1">



<title>homework-2</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">homework-2</h1>



<p><strong>BIS 557 Homework 2: Lia Lee</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(linearmodel)</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">#&gt; </span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">#&gt; Attaching package: &#39;linearmodel&#39;</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">#&gt; The following objects are masked from &#39;package:bis557&#39;:</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">#&gt; </span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">#&gt;     ridge_lambda_optimization, ridge_regression</span></span></code></pre></div>
<p><strong>1. CASL 2.11 Problem #5</strong></p>
<p>Consider the simple regression model with only a scalar x and intercept:</p>
<p><span class="math inline">\(y = \beta_0 + \beta_1 \cdot x\)</span></p>
<p>Using the explicit formula for the inverse of a 2x2 matrix write down the least squares estimators for <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span>.</p>
<p>Assuming that there are n observations, <span class="math inline">\(y = \beta_0 + \beta_1 x\)</span> where</p>
<p><span class="math display">\[
y = \left(\begin{array}{ccc} 
x_{11}&amp;\cdots&amp;x_{p1}\\
\vdots&amp;\ddots&amp;\vdots\\
x_{p1}&amp;\cdots&amp;x_{pn}\\
\end{array}\right)
\left(\begin{array}{c} 
\hat{\beta_0}\\
\hat{\beta_1}\\
\end{array}\right)
\]</span></p>
<p>In order to find the <span class="math inline">\(\hat{\beta}\)</span> estimators, we want to minimize <span class="math inline">\(|| y - x\beta||_2^2\)</span>. We can do this by looking first at <span class="math inline">\(f(b) = (y-bx)^2\)</span> and then setting <span class="math inline">\(f&#39;(b) = 2(y-bx)(-x) = 0\)</span> and solving for beta.</p>
<p>In matrix algebra notation, we have the following:</p>
<p><span class="math display">\[f&#39;(\beta) = -2 (X^T)(Y-X\beta) = 0\]</span> <span class="math display">\[X^T(Y-X\beta) = 0\]</span> <span class="math display">\[X^TY-X^TX\beta = 0\]</span> <span class="math display">\[X^Ty = X^TX\beta\]</span></p>
<p><span class="math display">\[\Longrightarrow \hat{\beta} = (X^TX)^{-1} X^T Y\]</span></p>
<p>Note here that <span class="math inline">\(\hat{\beta}\)</span> is a 2x1 matrix, <span class="math inline">\((X^TX)^{-1}\)</span> is a 2x2 matrix, <span class="math inline">\(X^T\)</span> is a 2xn matrix, and Y is a nx1 matrix, so the dimensionality works out.</p>
<p>Let’s solve for the first part: <span class="math inline">\(X^TX\)</span> the 2x2 matrix. In the notation below, always assume the summation goes from <span class="math inline">\(i = 1\)</span> to <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[X^TX = \left(\begin{array}{ccc} 
1&amp;\cdots&amp;1\\
\vdots&amp;\ddots&amp;\vdots\\
x_1&amp;\cdots&amp;x_n\\
\end{array}\right)
\left(\begin{array}{cc} 
1 &amp; x_1\\
\vdots &amp; \vdots\\
x_1 &amp; x_n\\
\end{array}\right) = 
\left(\begin{array}{cc} 
n &amp; \sum x_i\\
\sum x_i &amp; \sum x_i^2\\
\end{array}\right) \]</span></p>
<p>Next, let’s solve for the inverse of this matrix, i.e. <span class="math inline">\((X^TX)^{-1}\)</span> by using the definition of inverse of a matrix as follows:</p>
<p><span class="math display">\[\text{For A } = \left(\begin{array}{cc} 
a &amp; b\\
c &amp; d\\
\end{array}\right), A^{-1} = \frac{1}{|A|} \left(\begin{array}{cc} 
d &amp; -b\\
-c &amp; a\\
\end{array}\right)\]</span></p>
<p><span class="math display">\[\text{Therefore } (X^TX)^{-1} = \frac{1}{n\sum x_i^2 - \sum x_i^2} \left(\begin{array}{cc} 
\sum x_i^2 &amp; -\sum x_i\\
-\sum x_i &amp; n\\
\end{array}\right)\]</span></p>
<p>Next, we will solve for <span class="math inline">\(X^T Y\)</span> which is a (2xn)(nx2) = 2x1 matrix:</p>
<p><span class="math display">\[X^T Y = \left(\begin{array}{ccc} 
1 &amp; \cdots &amp; 1\\
\vdots &amp; \ddots &amp; \vdots\\
x_1 &amp; \cdots &amp; x_n\\
\end{array}\right) \left(\begin{array}{c} 
y_1 \\
\vdots\\
y_n \\
\end{array}\right) = 
\left(\begin{array}{c} 
\sum y_i \\
\sum x_i y_i\\
\end{array}\right)\]</span></p>
<p>Putting it all together, we have: <span class="math display">\[\hat{\beta} = (X^T X)^{-1} XT ^Y = \frac{1}{n\sum x_i^2 - (\sum x_i)^2} 
\left(\begin{array}{cc} 
\sum x_i^2 &amp; -\sum x_i\\
-\sum x_i &amp; n\\
\end{array}\right)
\left(\begin{array}{c} 
\sum y_i\\
\sum x_i \sum y_i\\
\end{array}\right)\]</span></p>
<p><span class="math display">\[ = \boxed{
\left(\begin{array}{c} 
\hat{\beta_0}\\
\hat{\beta_1}\\
\end{array}\right) = \frac{1}{n\sum x_i^2 - (\sum x_i)^2} 
\
\left(\begin{array}{c} 
\sum x_i^2 \sum y_i - \sum x_i (\sum x_i y_i)\\
-\sum x_i \sum y_i + n \sum x_i y_i\\
\end{array}\right)}\]</span></p>
<p><strong>4. Section 2.8 of CASL shows that as the numerical stability decreases, statistical errors increase. Reproduce the results and then show that using ridge regression can increase numerical stability and decrease statistical error.</strong></p>
<p>Note: The code below is modified from pg 30-31 of the CASL textbook.</p>
<p>To demonstrate the inverse relationship between numerical stability and statistical error, we will run the following simulation.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">library</span>(casl)</span>
<span id="cb2-2"><a href="#cb2-2"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span>; p &lt;-<span class="st"> </span><span class="dv">25</span> </span>
<span id="cb2-3"><a href="#cb2-3"></a>beta &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, p<span class="dv">-1</span>)) <span class="co">#create regression vector beta with 1st coordinate 1 and the rest 0</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), <span class="dt">ncol=</span>p) <span class="co">#data matrix X with randomly sampled normal variables</span></span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a>svals &lt;-<span class="st"> </span><span class="kw">svd</span>(X)<span class="op">$</span>d</span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="kw">max</span>(svals) <span class="op">/</span><span class="st"> </span><span class="kw">min</span>(svals) <span class="co">#calculating the condition # of the matrix X --&gt; (1.36)</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co">#&gt; [1] 1.357468</span></span>
<span id="cb2-9"><a href="#cb2-9"></a></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co">#generate y and see how close beta_OLS is to the true beta</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb2-12"><a href="#cb2-12"></a>count_errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</span>
<span id="cb2-14"><a href="#cb2-14"></a>  y &lt;-<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)</span>
<span id="cb2-15"><a href="#cb2-15"></a>  betahat &lt;-<span class="st"> </span><span class="kw">casl_ols_svd</span>(X,y)</span>
<span id="cb2-16"><a href="#cb2-16"></a>  count_errors[k] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((betahat <span class="op">-</span><span class="st"> </span>beta)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb2-17"><a href="#cb2-17"></a>}</span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="kw">mean</span>(count_errors) <span class="co">#typical error is small = 0.158</span></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="co">#&gt; [1] 0.1584034</span></span>
<span id="cb2-20"><a href="#cb2-20"></a></span>
<span id="cb2-21"><a href="#cb2-21"></a><span class="co">#replace 1st column of X with linear combo of 1st and 2nd column (--&gt; 2 X columns highly correlated) leading to higher condition # for matrix X^T X</span></span>
<span id="cb2-22"><a href="#cb2-22"></a></span>
<span id="cb2-23"><a href="#cb2-23"></a>alpha &lt;-<span class="st"> </span><span class="fl">0.001</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>X[,<span class="dv">1</span>] &lt;-<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>alpha) </span>
<span id="cb2-25"><a href="#cb2-25"></a>svals &lt;-<span class="st"> </span><span class="kw">svd</span>(X)<span class="op">$</span>d</span>
<span id="cb2-26"><a href="#cb2-26"></a><span class="kw">max</span>(svals) <span class="op">/</span><span class="st"> </span><span class="kw">min</span>(svals) <span class="co">#condition number high = 2000</span></span>
<span id="cb2-27"><a href="#cb2-27"></a><span class="co">#&gt; [1] 1940.213</span></span>
<span id="cb2-28"><a href="#cb2-28"></a></span>
<span id="cb2-29"><a href="#cb2-29"></a><span class="co">#rerunning simulation, we see that the error rate increased significantly</span></span>
<span id="cb2-30"><a href="#cb2-30"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb2-31"><a href="#cb2-31"></a>count_errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</span>
<span id="cb2-32"><a href="#cb2-32"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</span>
<span id="cb2-33"><a href="#cb2-33"></a>  y &lt;-<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)</span>
<span id="cb2-34"><a href="#cb2-34"></a>  betahat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(X), <span class="kw">crossprod</span>(X,y))</span>
<span id="cb2-35"><a href="#cb2-35"></a>  count_errors[k] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((betahat <span class="op">-</span><span class="st"> </span>beta)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb2-36"><a href="#cb2-36"></a>}</span>
<span id="cb2-37"><a href="#cb2-37"></a><span class="kw">mean</span>(count_errors)</span>
<span id="cb2-38"><a href="#cb2-38"></a><span class="co">#&gt; [1] 34.47601</span></span></code></pre></div>
<p>Now, let’s try using ridge regression. We want to show that the condition number decreases (stability increases) and statistical error decreases.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a>alpha &lt;-<span class="st"> </span><span class="fl">0.001</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>lambda &lt;-<span class="st"> </span><span class="fl">0.01</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>p), <span class="dt">ncol=</span>p) <span class="co">#redo X</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>X[,<span class="dv">1</span>] &lt;-<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>alpha) </span>
<span id="cb3-5"><a href="#cb3-5"></a>svals &lt;-<span class="st"> </span><span class="kw">svd</span>(X)<span class="op">$</span>d</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="kw">max</span>(svals) <span class="op">+</span><span class="st"> </span>lambda <span class="op">/</span><span class="st"> </span><span class="kw">min</span>(svals) <span class="op">+</span>lambda <span class="co">#condition number = 46</span></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="co">#&gt; [1] 46.6443</span></span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>check_errors &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</span>
<span id="cb3-12"><a href="#cb3-12"></a>  y &lt;-<span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)</span>
<span id="cb3-13"><a href="#cb3-13"></a>  betahat_ridge &lt;-<span class="st"> </span><span class="kw">solve</span>( <span class="kw">crossprod</span>(X) <span class="op">+</span><span class="st"> </span><span class="kw">diag</span>(<span class="kw">rep</span>(lambda, <span class="kw">ncol</span>(X))) ) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>y <span class="co">#change to beta_hat_ridge</span></span>
<span id="cb3-14"><a href="#cb3-14"></a>  check_errors[k] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((betahat_ridge <span class="op">-</span><span class="st"> </span>beta)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb3-15"><a href="#cb3-15"></a>}</span>
<span id="cb3-16"><a href="#cb3-16"></a></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="kw">mean</span>(check_errors)</span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="co">#&gt; [1] 1.728035</span></span></code></pre></div>
<p>Indeed, the conditional number has been reduced from 2000 to nearly 45, indicating an increase in numerical stability. In addition, the statistical error has dropped significantly from 34 to less than 2.</p>
<p><strong>5. Consider the LASSO penalty.</strong></p>
<p>Consider the LASSO penalty <span class="math inline">\(\frac{1}{2n} || Y - X \beta ||_2^2 + \lambda|| \beta ||_1\)</span>. Show that if <span class="math inline">\(|X_j^TY| \le n\lambda\)</span>, then <span class="math inline">\(\hat{\beta}_{LASSO} = 0\)</span>.</p>
<p>We want to start with <span class="math inline">\(f(\beta) = || y-X\beta ||^2_2 + \lambda |\beta| = \sum(y_i = (x_i \beta)^2 + \lambda |\beta|\)</span> and evaluate the two cases where <span class="math inline">\(\beta \ge 0\)</span> and <span class="math inline">\(\beta &lt; 0\)</span>. For the first case we will assume that <span class="math inline">\(\beta \ge 0\)</span>. It therefore follows that:</p>
<p><span class="math display">\[f(\beta) = \sum(y_i = (x_i \beta)^2 + \lambda \beta\]</span> <span class="math display">\[f&#39;(\beta) = -2 \sum x_i(y_i - x_i\beta) + \lambda = 0\]</span> <span class="math display">\[\Longrightarrow \sum x_i(y_i - x_i \beta) - \frac{\lambda}{2} = 0\]</span></p>
<p><span class="math display">\[\sum x_i y_i - \sum x_i^2 \beta - \frac{\lambda}{2} = 0\]</span></p>
<p><span class="math display">\[\sum x_i y_i - \frac{\lambda}{2} = \sum x_i^2 \beta\]</span></p>
<p>Assuming that <span class="math inline">\(\sum x_i^2 = 1\)</span> and solving for <span class="math inline">\(\beta\)</span>, we get <span class="math inline">\(\hat{\beta} = \sum x_i y_i - \frac{\lambda}{2}\)</span>. Which only holds if <span class="math inline">\(\sum x_i y_i &gt; \frac{\lambda}{2} &gt; 0\)</span>.</p>
<p>Reformatting this in matrix algebra notation, the following is equivalent:</p>
<p><span class="math display">\[L = \frac{1}{2n} || Y - X\beta ||_2^2 + \lambda \beta\]</span> <span class="math display">\[\frac{dL}{d\beta} = \frac{2}{2n} (-X^t)(Y-X\beta) + \lambda = 0\]</span> <span class="math display">\[(-X^T)(Y-X\beta) + n\lambda = 0\]</span> <span class="math display">\[-X^TY + X^T X \beta + n\lambda = 0\]</span> <span class="math display">\[X^T X \beta = X^T Y - n \lambda\]</span></p>
<p><span class="math display">\[\boxed{\hat{\beta} = (X^TX)^{-1} [X^TY - n\lambda] = X^TY - n\lambda}\]</span></p>
<p>Given that <span class="math inline">\((X^T X) = I\)</span>. This tells us that for <span class="math inline">\(\beta \ge 0\)</span>, the value <span class="math inline">\(\hat{\beta} = X^T Y - n\lambda\)</span> achieves the minimum value of of <span class="math inline">\(L(\beta)\)</span>. Picturing <span class="math inline">\(L(\beta)\)</span> as a shape of an upward parabola, with minimum at <span class="math inline">\(\hat{\beta} = X^T Y - n\lambda\)</span>, if <span class="math inline">\(X^T Y - n\lambda &lt; 0\)</span>, or if 0 is to the right of <span class="math inline">\(X^T Y - n\lambda\)</span>, then the minimum value of is 0. If however, <span class="math inline">\(X^T Y - n\lambda &gt; 0\)</span>, or if 0 is to the left of <span class="math inline">\(X^T Y - n\lambda\)</span>, then <span class="math inline">\(X^T Y - n\lambda\)</span> is indeed greater than 0 (matching our condition) and the minimum. However, at the beginning, we are given that <span class="math inline">\(|X^TY| &lt; n\lambda\)</span>, so it must be true the first case scenario, where <span class="math inline">\(X^T Y - n\lambda &lt; 0\)</span> and that <span class="math inline">\(\hat{\beta}_{LASSO} = 0\)</span>. If we solve <span class="math inline">\(\hat{\beta}_{LASSO}\)</span> for the other condition where <span class="math inline">\(\beta &lt; 0\)</span>, we achieve a similar result where <span class="math inline">\(\hat{\beta} = X^T Y + n\lambda\)</span> by directionality. Now, if we have the case where <span class="math inline">\(\hat{\beta} = X^T Y + n\lambda &gt; 0\)</span>, we get that <span class="math inline">\(\hat{\beta} = 0\)</span> by the initial constraint. If we have <span class="math inline">\(\hat{\beta} = X^T Y + n\lambda &lt; 0\)</span>, the the minimum is <span class="math inline">\(X^T Y + n\lambda\)</span>. A similar set of solutions is provided in pg 184 of the CASL textbook. Note that the above work was proved for a 1-dimensional case, but it can be similarly applied for a multidimensional case, i.e. given <span class="math inline">\(X_j^T Y &lt; n\lambda\)</span> then <span class="math inline">\(\hat{\beta}_{LASSO} = 0\)</span> for <span class="math inline">\(j=1, \cdots, p\)</span>.</p>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
